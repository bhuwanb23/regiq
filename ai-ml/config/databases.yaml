# REGIQ AI/ML Database Configuration
# Configuration for all database connections and data storage

# =============================================================================
# PRIMARY DATABASES
# =============================================================================
databases:
  # Local SQLite for development and initial setup
  sqlite:
    database_path: "./data/regiq_local.db"
    echo: false
    pool_recycle: 3600
    
    # Tables for structured data
    tables:
      users: "user_profiles"
      auth: "authentication_logs"
      bias_reports: "model_bias_analysis"
      compliance_scores: "regulatory_compliance"
      audit_trails: "system_audit_logs"
      
  # PostgreSQL for production
  postgresql:
    host: "${POSTGRES_HOST:-localhost}"
    port: "${POSTGRES_PORT:-5432}"
    database: "${POSTGRES_DB:-regiq_ai_ml}"
    username: "${POSTGRES_USER:-regiq_user}"
    password: "${POSTGRES_PASSWORD:-regiq_password}"
    ssl_mode: "prefer"
    pool_size: 10
    max_overflow: 20
    
    # Same table structure as SQLite
    tables:
      users: "user_profiles"
      auth: "authentication_logs"
      bias_reports: "model_bias_analysis"
      compliance_scores: "regulatory_compliance"
      audit_trails: "system_audit_logs"
      regulations: "regulatory_documents"
      alerts: "compliance_alerts"
      simulations: "risk_simulation_results"
      reports: "generated_reports"

# =============================================================================
# VECTOR DATABASES (for RAG) - Local First
# =============================================================================
vector_databases:
  # Local ChromaDB for development
  chroma:
    persist_directory: "./data/chroma_db"
    collection_name: "regulatory_documents"
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
    
  # FAISS for local vector search
  faiss:
    index_path: "./data/faiss_index"
    dimension: 384
    index_type: "IndexFlatIP"
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"

# =============================================================================
# CACHE DATABASES
# =============================================================================
cache:
  redis:
    host: "${REDIS_HOST:-localhost}"
    port: "${REDIS_PORT:-6379}"
    password: "${REDIS_PASSWORD}"
    db: 0
    max_connections: 50
    
    # Cache configurations
    ttl:
      model_predictions: 3600      # 1 hour
      regulatory_summaries: 86400  # 24 hours
      bias_reports: 604800         # 7 days
      risk_simulations: 259200     # 3 days

# =============================================================================
# SEARCH & ANALYTICS
# =============================================================================
search:
  elasticsearch:
    hosts:
      - "${ELASTICSEARCH_HOST:-localhost}:${ELASTICSEARCH_PORT:-9200}"
    username: "${ELASTICSEARCH_USER:-elastic}"
    password: "${ELASTICSEARCH_PASSWORD}"
    use_ssl: true
    verify_certs: true
    
    # Indices for different data types
    indices:
      logs: "regiq-ai-ml-logs"
      metrics: "regiq-performance-metrics"
      alerts: "regiq-system-alerts"
      user_actions: "regiq-user-analytics"

# =============================================================================
# FILE STORAGE - Local First
# =============================================================================
file_storage:
  local:
    base_path: "./data/storage"
    
    # Storage paths
    paths:
      models: "models/"
      datasets: "datasets/"
      reports: "reports/"
      backups: "backups/"
      uploads: "uploads/"
      
    # File size limits (in MB)
    limits:
      max_file_size: 100
      max_dataset_size: 1000
      
  # Optional: Google Cloud Storage for production scaling
  google_cloud_storage:
    enabled: false
    bucket_name: "${GCS_BUCKET_NAME}"
    project_id: "${GCP_PROJECT_ID}"
    credentials_path: "${GOOGLE_APPLICATION_CREDENTIALS}"

# =============================================================================
# DATA PIPELINE CONFIGURATION
# =============================================================================
data_pipeline:
  batch_processing:
    schedule: "0 2 * * *"  # Daily at 2 AM
    batch_size: 1000
    max_retries: 3
    
  streaming:
    kafka:
      bootstrap_servers: "${KAFKA_BOOTSTRAP_SERVERS}"
      topics:
        regulatory_updates: "regulatory-updates"
        model_predictions: "model-predictions"
        bias_alerts: "bias-detection-alerts"
        
  etl_jobs:
    regulatory_scraping:
      frequency: "daily"
      sources: ["sec.gov", "eba.europa.eu", "bis.org"]
      
    model_retraining:
      frequency: "weekly"
      trigger_conditions: ["data_drift", "performance_degradation"]

# =============================================================================
# BACKUP & DISASTER RECOVERY
# =============================================================================
backup:
  postgresql:
    frequency: "daily"
    retention_days: 30
    compression: true
    
  mongodb:
    frequency: "daily"
    retention_days: 30
    incremental: true
    
  file_storage:
    frequency: "weekly"
    cross_region_replication: true
    versioning: true

# =============================================================================
# MONITORING & ALERTING
# =============================================================================
monitoring:
  database_health:
    connection_pool_threshold: 80
    query_performance_threshold: 1000  # ms
    disk_usage_threshold: 85           # %
    
  data_quality:
    completeness_threshold: 95         # %
    accuracy_threshold: 90             # %
    freshness_threshold: 24            # hours
    
  alerts:
    channels: ["email", "slack", "pagerduty"]
    escalation_policy: "critical_issues"

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================
security:
  encryption:
    at_rest: true
    algorithm: "AES-256"
    key_rotation_days: 90
    
  access_control:
    network_security: "vpc_only"
    ip_whitelist: ["10.0.0.0/8", "172.16.0.0/12"]
    
  audit:
    log_all_queries: true
    sensitive_data_masking: true
    retention_period: 2555  # 7 years

# =============================================================================
# ENVIRONMENT CONFIGURATIONS
# =============================================================================
environments:
  development:
    debug: true
    log_level: "DEBUG"
    auto_migrations: true
    
  staging:
    debug: false
    log_level: "INFO"
    performance_testing: true
    
  production:
    debug: false
    log_level: "WARNING"
    high_availability: true
    read_replicas: 2
